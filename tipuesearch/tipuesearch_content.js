var tipuesearch = {"pages": [

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark231'>CLEAN CLIFcode Image</h1>", "text": "CLEAN CLIFcode Image", "tags": "", "url": "page5501091875.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark1'>TtDT - Report</h1>", "text": "Work In Progress 2\n\nTtDT - Report - IntroductionTtDT - Report - A resilient approachTtDT - Report - A resilient, transparent bCLEARer pipeline architectureTtDT - Report - bCLEARer's pipeline or pipe-and-filter architectureTtDT - Report - bCLEARer's nested gated pipeline architectureTtDT - Report - bCLEARer pipeline's three nesting levelsTtDT - Report - bCLEARer pipeline - general designTtDT - Report - Building resilient transformation transparency into the bCLEARer pipelineTtDT - Report - Building resilient dataset transformation transparencyTtDT - Report - Building resilient data item transformation transparencyTtDT - Report - AppendicesTtDT - Report - Appendix - Process: principles versus rulesTtDT - Report - Appendix - bH - bHashing and bSummingTtDT - Report - Appendix - cohesion and couplingTtDT - Report - Appendix - separation of concerns principleTtDT - Report - Appendix - immutability and idempotence principleTtDT - Report - Appendix - single-transformation (responsibility) principle (STP)TtDT - Report - Appendix - Aggregated S(ingle) S(ource) O(f) T(ruth)TtDT - Report - Appendix - design patterns and anti-patternsTtDT - Report - Appendix - Glossary of Major TermsTtDT - Report - Appendix - Reference IconographyTtDT - Report - Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' ArchitectureTtDT - Report - ReferencesTtDT - Report - Acknowledgements", "tags": "", "url": "page5766283265.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark2'>TtDT - Report - Introduction</h1>", "text": "The bCLEARer framework facilitates taking information on a digital journey, curating its digital evolution into forms more suited for computing. This journey involves a series of transformations that both: salvage the information in existing artefacts \u2013 transforming the way in which it is stored \u2013 making it more accessible, more computer readable and more amenable to later transformations (digitisation), andtransform the information, evolving and improving it (digitalisation).In any evolutionary process, there needs to be a selection of the adaptions (in this case, transformations) that are fitter. In the controlled evolution in the bCLEARer environment, selection is reinforced by inspection which relies upon the transformations being visible for inspection. If the transformations are transparent, this enables one to both easily see where they go wrong and fix them \u2013 and see where they go right and engender trust in them.Hence the bCLEARer framework has over time developed ways of making the transformations transparent; ways of identifying, tracking, tracing and testing what information there is and how it is transformed. This report provides an overview of the foundation for these. Its aim is to outline core practices that, when adopted, help build trust in the bCLEARer digital transformations.These core practices are outlined in the body of the report. Further details, including more technical matter and reference material is relegated to Appendices. This includes a glossary of the main terms (TtDT - Report - Appendix - Glossary of Major Terms) and a reference iconography (TtDT - Report - Appendix - Reference Iconography).", "tags": "", "url": "page5765071213.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark3'>TtDT - Report - A resilient approach</h1>", "text": "The bCLEARer approach is architected to uncover opportunities for digital evolution and to rapidly exploit them. The initial stages of the approach are ones of discovery, where the process evolves to meet the emerging requirements and so usually involves radical change. Given the need to support rapid evolutionary change, including radical change, the bCLEARer approach needs to be resilient, to thrive rather than collapse when faced with change. Any resilient system must strike a good balance between stability and flexibility. The bCLEARer approach has evolved to be resilient \u2013 to facilitate radical change. The aim of this report is to provide a foundation for the resilient inspection practices that have emerged in the bCLEARer approach over the last three decades. These exploit ways of identifying, tracking, tracing and testing what information there is and how it is transformed. Adopting these core practices helps to build trust in the bCLEARer digital transformations.Right balance of principles and rules Any resilient system must strike a good balance between stability and flexibility. It must have stable elements that persist through change and flexible elements that can adapt to changes. From an ecosystem perspective, the stable infrastructure can be regarded as the environment in which the transformations unfold and evolve.This good balance of stability and flexibility is needs to be reflected in framing the approach within the right balance of principles and rules \u2013 particularly algorithmic rules -, see https://borocvi.atlassian.net/l/cp/kSZJbnMy. Where principles describe the broad aims and trust in the commitment of those executing them, whereas rules specify narrow, potentially rigid, constraints upon what can and cannot be done. RulesRules aim to provide a structure that is, in theory, sufficient for resolving a particular type of case. They might seem to provide certainty and a clear standard of behaviour. They might seem to be easier to apply consistently. But it is difficult for rules to be congruent with their purpose, to achieve what it is they are intended to achieve. Rules are based upon a \u201Cbest guess\u201D as to the future. The rulemaker has to anticipate how the rule will be applied in the future. But new situations may arise that were not expected or known about when the rule was written. The rule may be interpreted and applied in ways that were not intended or anticipated by the writer. In practice, this not only leads to gaps, inconsistencies, rigidity, but is prone to degeneration into \u201Ccreative compliance\u201D (where the letter is followed but not the spirit). This creates a need for constant adjustment to new situations. This can then then lead to a \u2018ratchet syndrome\u2019, where more rules are created to address new problems or close new gaps, creating more gaps and so on.PrinciplesPrinciples are typically more general and focus on purpose rather than process. This makes them more flexible without sacrificing congruence with their purpose. The execution of both rules and principles rely upon being understood, a shared understanding in the case of teams. However, in the case of principles there is typically a need for a wider and deeper understanding to interpret the principles in the way intended, or even suitably extend the interpretation to novel situations. Principles and rulesGiven the variety of situations in which bCLEARer is deployed and the requirement for rapid evolution and the associated radical change, rules by themselves are not the right tool for the job. Principles have a better initial fit for the bCLEARer situation. So there is no choice but to start with a principle-based approach. However, the requirement for a repeatable, scalable process means that in the end, the bCLEARer process needs to be developed as largely (computer-)executable rules. The aim of this report is to describe the principles that underlie the bCLEARer approach to resilient transparency as well as start to develop some understanding of the kinds of executable rules that should be developed.Report structureThe approach is outlined in the body of the report, in two main sections:A resilient, transparent bCLEARer pipeline architectureBuilding resilient transformation transparency into the bCLEARer pipelineThe first section focuses on the pipeline architecture of the overall approach. This is the infrastructure that provides the stability that any resilient system requires to remain flexible in the face of change. The second section describes how transformation transparency is built into the bCLEARer pipelines based upon notions of identity. It focuses on how to build in transparency from the bottom up.  Then how to exploit this to identify, track, trace and test what information there is and how it is transformed. ", "tags": "", "url": "page5769560149.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark9'>TtDT - Report - A resilient, transparent bCLEARer pipeline architecture</h1>", "text": "To help ensure resilience, it is important that the architecture of the system strike the right balance between stability and flexibility - and provide the stability within which the system can flex. In the case of the bCLEARer approach, it is the stable element that persists through change providing the infrastructure in which the transformations unfold and evolve.The bCLEARer approach\u2019s architecture also needs to encourage successful evolution. In particular, it needs to be transparent, actively encouraging the inspection of the evolutionary transformations along the digital journey. The architecture described here has evolved over time in response to the requirement for a stable infrastructure that supports the rapid, transparent evolution of the pipeline.There are three core aspects that characterise the bCLEARer architecture:firstly, the overall \u2018pipeline\u2019 or \u2018pipe and filter\u2019 architecture (additional material in the appendix: https://borocvi.atlassian.net/wiki/spaces/SB/pages/5773197350/Appendix+-+Pipe+and+Filter+Architecture). secondly, the bCLEARer gated nesting structure of the pipes and filters.thirdly, the three broad nesting levels.Then there is the general approach to developing the design within the architecture.These are described in the following sections. Sub-PagesTtDT - Report - bCLEARer's pipeline or pipe-and-filter architectureTtDT - Report - bCLEARer's nested gated pipeline architectureTtDT - Report - bCLEARer pipeline's three nesting levelsTtDT - Report - bCLEARer pipeline - general design", "tags": "", "url": "page5766316210.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark11'>TtDT - Report - bCLEARer's pipeline or pipe-and-filter architecture</h1>", "text": "IntroductionAs is often noted in the literature (see, for example, the extract in Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' Architecture), data transformation systems typically have a \u2018pipeline\u2019 or \u2018pipe-and-filter\u2019 architecture. ('Pipe-and-filter' is the original name for this architecture, though \u2018pipeline\u2019 is a more common name nowadays.) The bCLEARer approach is a data transformation process and so, unsurprisingly, is implemented with a pipeline architecture \u2013 giving rise to a bCLEARer pipeline. Large pipelines are typically given a hierarchical, nesting structure to facilitate management - the specifics of this are discussed in the next section. In this section, we look at the pipeline architecture, including how it can be nested.Visualising the pipeline flowThis architecture consists of a sequence of processing components, arranged so that the output of each component is the input of the next one creating a \u2018flow\u2019. A simple visualisation of a pipeline flow is given below:The pipeline architecture has, as the 'pipe-and-filter' name suggests, a series of pipe and filter components, where pipes pass data to and from filters that transform the passed data - the pipeline flow. As well as the general inter-filter pipes, there are two specific types of pipes, the start and end pipes; a data source (pipe) to feed the pipeline and a data target (pipe) to persist the transformed data. As illustrated in the figure above, these pipes are typically adorned with a dataset collection icon (as shown below).Process timeConventionally, the pipeline flow - pipes passing data and filters transforming it - is shown across the diagram from left to right. This is made explicit in some diagrams with the use of a \u2018process time\u2019 arrow, as shown below.Evolutionary timeThe flow itself will typically evolve over time, pipes and filters may be added and removed - or changed. One needs be able to represent this evolution in diagrams.  This is typically represented as a series of snapshots of the pipeline (process) arranged down the page with an arrow from top to bottom showing the direction of evolution \u2013 a pro-forma example of this is in the figure below.Pipeline componentsThe pipeline has two core components, 'filters' and 'pipes' (including the data source and data target pipes).FiltersFilters are components\u00A0that transform ('filter') data\u00A0that is received as an input via pipe connectors. The icon for a filter is shown below:PipesPipes are the connectors for filters. The role of a pipe is to pass messages, or information, to and from filters. The flow is unidirectional, and, when needed in an implementation, the data is persisted until the filter processes it.\u00A0The icon for a pipe is shown below:Pipes adorned with dataAs noted earlier, there is a data source pipe at the start of the pipeline to feed it and a data target pipe at the end of the pipeline to persist the transformed data. And these are usually adorned with a dataset collection icon. All pipes, not just the start and end pipes, transport data. Optionally, this point can be highlighted through the use of a data icon (in this case, the dataset collection icon) on all pipes in the diagram including those pipes inside the pipelines \u2013 as shown below for pipes 2 and 3.Filters and their pipesFilters always have an input pipe and an output pipe, as shown in the following diagram:Acyclic pipeline flowA filter\u2019s pipe cannot flow to itself, either directly or indirectly. The flow is acyclic \u2013 with no cycles. This does not inhibit reuse, as the same processing may be reused in different filters.Multiple pipes to and from different filtersA filter can have several input pipes and several output pipes, as shown in the following diagram:Merging and splitting pipelinesFilters with multiple input and output pipes can be organised into pipeline flows that split and then merge \u2013 as shown below. Multiple pipes between the same filtersNone of a filter's pipes should flow to the same destination \u2013 where this happens, the pipes should be encapsulated - as shown in the diagram below.Pipes: single or multiple filter inputs Typically a pipe will have a single filter output fed by a single filter input. However, there can be cases where it is important to record that the same data is fed from one filter to many other filters. In these cases, the pipe is shown with a single filter output feeding multiple filter inputs. Pro-forma examples of these two cases are in the figure below.NestingAny subset of filters in a pipeline forms a sub-pipeline, however, it can be useful to distinguish between pipelines that are connected and disconnected  \u2013 see below.This makes it simple to organise a sub-pipeline into a nested pipeline \u2013 as shown below.One can view the nesting in a diagram - as shown below.Nesting \u2013 pipe encapsulation Where a nesting is going to encapsulate a number of filters, one consequence may be that a filter outside the nesting, whose pipes previously fed into multiple filters, now feeds into (or is fed from) a single encapsulated filter. In this case, the rule 'Multiple pipes between the same Filters' mentioned above comes into play, and the pipes need to be encapsulated to ensure in the nested pipeline there are not multiple pipes between the same (nested) filters - as shown in the figure below (as an aside, note the sub-pipeline is technically disconnected).We can see the original multiple pipes in the nesting diagram - as shown in the figure below.", "tags": "", "url": "page5773230168.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark28'>TtDT - Report - bCLEARer's nested gated pipeline architecture</h1>", "text": "IntroductionbCLEARer pipelines are often quite large, and as noted earlier, large pipelines are typically given a hierarchical, nesting structure to facilitate management. In this section, we look at the specific nesting strategies adopted by bCLEARer pipelines. Within the overall pipeline architecture, the bCLEARer pipeline has a gated nesting structure. The first sub-section looks at the nesting, the second at the gating.The bCLEARer pipeline\u2019s multi-level nesting structureThe bCLEARer pipeline\u2019s multi-level nesting structure is described in this section.Multi-level nestingAs noted earlier, within a pipeline architecture, one can encapsulate sub-pipelines as filters (with pipes). These encapsulated components can themselves be nested, creating a multi-level nesting (breakdown) structure in the pipeline. One can view the encapsulation structure in a nesting diagram - as shown below. The final nesting is of all the filters into a single overall-pipeline-level filter. From the perspective of this top level final pipeline, the nesting is a series of decompositions. These can be a fixed series of decompositions into levels, where each decomposition takes the units at one level and divides them into units at the next level. This breaks down the overall process into levels, with base (undivided) units at the bottom level \u2013 as shown in the figure below. bCLEARer nesting decomposition levelsThe bCLEARer pipeline is composed of three broad levels of nested pipelines (described in more detail later), based on the type of base filter:bCLEARer nesting decomposition levels\n(Domain) thin slicesthe base unit of the breakdown of scope into domains bCLEARer stagesthe base unit of the stages of the bCLEARer digital journeybUnitsthe base unit of the bCLEARer pipeline architecture, composed of the bUnit filter and its associated bUnit pipessource: bCLEARer nesting decomposition levels\nThis can be visualised as a three-level breakdown.Within each level there may be further unrestricted secondary nesting, with as many levels of nesting as is useful.Nested pipeline\u2019s data stage gatesThe nested pipeline data stage gates are described in this section.The data stage gatesWhere appropriate, the nested pipelines are designed with their input and output pipes as data stage gates. Typically, bCLEARer stage and thin slice pipelines have these gates. This dual (input and output) gate design for nested pipelines enables the original and transformed data to be inspected and compared. To assist with this, the design of an output gate's data also aims to clean it sufficiently to provide a SSOT (TtDT - Report - Appendix - Aggregated S(ingle) S(ource) O(f) T(ruth)) snapshot of its state at this stage of its digital journey. Where an input gate is an output gate of the previous process, it will also be in a SSOT snapshot. And in so doing, one can make the journey's transformations visible by comparing snapshots. In the bCLEARer process, these stage-gates are not decision points (as they are in some waterfall processes); though where data fails an inspection, it may be held back if required. The focus is on inspection not decision in an agile iterative process. Establishing data gates for nested pipelines, especially SSOT data gates, is a very cost effective way of creating inspection points that can greatly simplify improving and maintaining quality. It helps, for example, in the identifying of the source of problems: finding the first gate at which a problem appears, isolates between which gates it arose.  One of the drivers in the design of the nesting structure is ensuring the gaps between gates are sufficiently small to make finding problem data easy. Designing a gateHence, gates are designed into the process. The design involves two layers of nesting. Given a pipeline that needs to be gated at one end, one needs to create a pipe that aggregates all the data flowing through that end of the pipeline. One way of doing this is creating a nesting that encapsulates all the filters with pipes that travel outside the pipeline. This will encapsulate the outgoing pipes into a single (aggregated) pipe,  an example of this is given in the figures below.In this figure, the filters are just encapsulated. This results in the unaggregated output of two pipes \u2013 pipe 5 and pipe 6.In this figure, the pipes are encapsulated first and then the filters. This results in the aggregated output of a single pipe \u2013 pipe 5&amp;6.This diagram includes the gate icon, as shown below.The data icon is typically adorned with a gate icon if the data is gated, as shown below.As usual, we can look at a nesting diagram to see the lower level structure hidden in the simple pipeline diagram - as shown in the figure below.", "tags": "", "url": "page5773656071.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark36'>TtDT - Report - bCLEARer pipeline's three nesting levels</h1>", "text": "IntroductionIn the previous section, we noted that bCLEARer pipelines are typically quite large and could benefit from a hierarchical, nesting structure. In this section we look in more detail at the levels of this nesting structure. This is an essential part of the stable bCLEARer infrastructure, the environment, within which the transformations evolve. The bCLEARer pipeline\u2019s primary nesting is based upon three broad levels (shown below). Within each level there may be further secondary nesting, allowing for as many levels of nesting as is useful at each level.bCLEARer nesting decomposition levels\n(Domain) thin slicesthe base unit of the breakdown of scope into domains bCLEARer stagesthe base unit of the stages of the bCLEARer digital journeybUnitsthe base unit of the bCLEARer pipeline architecture, composed of the bUnit filter and its associated bUnit pipessource: bCLEARer nesting decomposition levels\nSometimes it is conceptually cleaner to describe the hierarchy from the bottom up starting from the base unit. In practice, the design often starts with the overall scope and decomposes this into base units. We follow this design practice in this section, where we describe this nesting by level from the bCLEARer Pipeline down.(Domain) thin slicesAt the first, domain, level of decomposition the bCLEARer pipeline is decomposed into thin slices. The bCLEARer Pipeline can cover one or more domains. With any sizeable domain (or domains), it makes sense to divide this into sub-pipelines focussing on smaller domain-based chunks . Where there are dependencies between the chunks, these need to be recognised in the bCLEARer pipeline flow. At the domain level, the base unit of chunking is the thin slice - this is where the domain decomposition stops. If it is useful, when the domain is large and will have a lot of thin slices, the decomposition can process in stage where the initial decompositions are into thin slice sets \u2013 pipelines composed of thin slices.Thin slice sets When designing the nesting structure for the domain, it often makes sense to start with the overall bCLEARer pipeline and make a series of decompositions down until one reaches the level of thin slices. Where these decompositions are driven by domain considerations. A pro-forma example of a first stage decomposition of a bCLEARer pipeline into thin slice sets (filters), where the flow is ordered by pipes is shown in the figure below.Gates are placed at the start and end of each thin slice set \u2013 as shown in the figure below using the dataset collection icon. This figure is based upon a real example, which involved two systems: GSAP and sigraph. The first two thin slice sets transformed the two systems in isolation and the third thin slice set merged and transformed the data from the two initial thin slice sets. Systems and their sub-systems often form natural boundaries for thin slice sets.Thin slicesEach thin slice set is further broken down until the base-unit thin slices are reached. The breakdowns are also ordered using pipes, as shown in the pro-forma nesting diagram in the figure below.Again, gates are placed at the start and end of each thin slice. This is shown in a real example in the nesting diagram in the figure below. This figure shows the merging of data within thin slice sets as well as across thin slice sets. This is not unusual, often a significant amount of the bCLEARer pipeline deals with the merging of data from different sources.bCLEARer stagesAt the next level of decomposition, the thin slice pipelines decompose into bCLEARer stages. These stages mark the stage of digital journey the data is on. Where there are many such stages, it can be useful to nest them into sub-pipelines. This is described below.The need to mark the stage of digital journey the data is on leads to a major constraint upon bCLEARer stages \u2013 they should flow in a sequence that reflects the journey. The sequence is well established (see first figure below).The bCLEARer stages are gated \u2013 as shown in the figure below.There is significant scope for organising these stages into a series of sequences \u2013 see example below, where one sequence follows another.Also, the thin slice pipeline will typically evolve during a project \u2013 as shown in the example below.Sometime the pipeline involves manual work. As manual work interferes with running, scaling and costs, the aim is as far as possible to automate this work. Where it cannot be automated, it is often a good idea to restrict it to the Load stage, moving the manual work to the early stages in the sequence. In bCLEARer stage diagrams we usually mark the stages that involve manual work, using an icon \u2013 this is visible in the Load stage in the diagram above. The manual icon is shown below.bUnitsThe bCLEARer stage pipelines decompose into bUnits \u2013 the base unit of the bCLEARer pipeline, these are not decomposed any further. As at the other levels, these can be organised into secondary levels. The pipes in bUnits work at the finer-grained level of datasets rather than dataset collections. In diagrams, the bUnit pipes are adorned with the dataset icon, as shown below.As the bCLEARer stages are gated, the bUnits need to be designed to accommodate the gates \u2013 this is shown as start and end bUnits in the figure below, as are the finer-grained datasets.Sometimes, unavoidably, a bUnit will be manual. This is marked using the manual icon \u2013 there is an example of this in the diagram above. bUnit - base unit for transformation, difference and identityIn the bUnit pipeline architecture, the bUnit pipes are the base units of identity and difference, and the bUnit filters the base units of transformation. In this architecture, the pipes transport data, the data is not transformed - so it is immutable stage of the data through the flow. The flow can then be seen as a sequence of bUnit immutable stages, where any transformation is located in the filters linking the stages.", "tags": "", "url": "page5766545422.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark44'>TtDT - Report - bCLEARer pipeline - general design</h1>", "text": "IntroductionThis section deals with the general design of the bCLEARer pipeline to facilitate transparency. The bCLEARer pipeline is designed as a sequence of nested filters - the pipeline flow. It is important that this flow is not only transparent, open to inspection, but also resilient in the face of change. Hence we discuss the general design principles that will encourage this.Design principlesWe have found it useful to design the bCLEARer pipeline with a view to reducing the cost of evolving the pipeline - especially radically evolving - to take advantage of the opportunities that emerge on the journey of discovery. There are many well-known principles and techniques to achieve this. For example, focusing on building in modularity not only increases maintainability and reusability, but also refactoring and extensibility. These are well-documented, and we aim to point to these resources here and in the associated appendices. However, our focus here is on the less well-documented goal of facilitating transparency. One core central motivation for this is that delivering transparent transformations, and so enabling easy inspection, is a solid basis for improving the value of, as well as enhancing the trust in, the transformations. The aim is to systematise this inspection so reducing the effort need to develop and maintain it during the rapid evolution of the pipeline. In the following sections, we aim to highlight here with some examples ways we can use the many well-known principles to better achieve our more focused goals. We start with a look at the general approach to decomposition, and then look at two principles:separation of transformation concerns immutability and idempotenceThere is a significant literature on these principles, so here we outline them and give references for further reading in the associated appendices.General approach to decompositionIn previous sections, we have visualised the modular structure of pipeline flow in terms of a flowchart to provide a picture of its structure. However, we need a different approach to decomposing it into filters. To reinforce this point, consider David L. Parnas\u2019s classic Communications of the ACM paper On the Criteria To Be Used in Decomposing Systems into Modules (1972) where he makes this point, proposing an alternative to simple flowchart decomposition saying: \u201CWe have tried to demonstrate by these examples that it is almost always incorrect to begin the decomposition of a system into modules on the basis of a flowchart. We propose instead that one begins with a list of difficult design decisions or design decisions which are likely to change. Each module is then designed to hide such a decision from the others.\u201DIn the case of bCLEARer, the design decisions should consider, at the least, the various different kinds of information transformation. At the lowest level, the breakdown should end up with bUnits that deal with a single transformation - bUnits should not handle more than one transformation. This could be regarded as a \u2018single-transformation principle\u2019 - alluding to the single-responsibility principle. For more details see: Appendix - Single-transformation (responsibility) principle (STP).Principle: separation of transformation concerns The separation of concerns (described in more detail in the Appendix (TtDT - Report - Appendix - separation of concerns principle) is similar in many respects to single-transformation principle (see: Appendix - Single-transformation (responsibility) principle (STP)). From a concerns perspective, the different types of transformation can be seen as different concerns, and so should be separated.  As a first stage, this leads to base bUnits with a single concern. This enables cleaner tracing of transformations, as it also separates out the contributors to the transformation. This can lead to a significant number of base Units. When organising these base bUnits into larger modules it is useful to consider principles of cohesion and coupling (there are more details on this in TtDT - Report - Appendix - cohesion and coupling). Cohesion is a measure of functional closeness - and a good modular design will have high cohesion where the functional closeness is high inside the modules and low outside. Coupling is a measure of interdependence. A good modular design will have low coupling between modules and higher coupling inside the module. This is illustrated visually in the figure below.Principle: immutability and idempotenceImmutability and idempotence are about how transformations are handled.  Immutable data\u2019s content cannot be modified after it is created. An idempotent process can be run multiple times with the same input without changing the output. The pipeline architecture promotes idempotent processes (filters) and immutable datasets.We can illustrate this by comparing interactive and batch processing. The data updated by interactive processing is typically not immutable - the process changes its content. Whereas the data created by batch processing - the common pipeline process - is typically immutable. This is shown in the graphic below. So, adopting immutable data and idempotent processes (filters) is a natural choice for a pipeline architecture. And adopting it has several advantages. The immutability simplifies auditability, making transformations easier to track.  The idempotence makes rerunning the processes safer and simpler. For more on these topics see the appendix: TtDT - Report - Appendix - immutability and idempotence principle.", "tags": "", "url": "page5775163422.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark50'>TtDT - Report - Building resilient transformation transparency into the bCLEARer pipeline</h1>", "text": "IntroductionThis section describes how resilient transformation transparency is built into the bCLEARer pipelines. The focus is on building in transparency from the bottom up, starting with the bUnits in the bCLEARer stages. This transparency is then inherited up the levels of the bCLEARer pipeline. The building is done in similar ways at two levels of data granularity:  dataset, anddata item.The sub-sections describe these two levels.In each subsection the following topics are covered:firstly, a general notion of algorithmic identity, from which difference and so transformation can be established, andsecondly, how to implement transparency in the individual bCLEARer stage pipelines through tracking, tracing and testing transformations.Algorithmic identityFor both levels of granularity (dataset, and data item), there are two core types of  identity:data identity, anddata content immutability (stage) identity.These are both based upon data item identities to facilitate their algorithmic processing.As discussed earlier, in the bCLEARer pipeline architecture, data\u2019s bUnit (pipe) stages are necessarily content immutable. This enables us to build the two core types of algorithmic identity from the base bUnit stage identity. Together these give the six types of transformation shown below.data identitydata content immutability (stage) identitybUnit (stage) identitydatasetdataset (item) identitydataset (item) content immutability (stage) identitydataset bUnit (stage) identitydata itemdata item identitydata item content immutability (stage) identitydata item bUnit (stage) identityidentity transformation typesOne can also keep a rough track on identity through counts if one wishes. This is a much weaker version of data identity and so we do not discuss it here.Coinciding identitybCLEARer has adopted an extensional notion of identity. This leads to cases where data identities (extensionally) coincide. To help us visualise the identities, we associate colours with them as shown below.The three life histories in the figure below illustrate this. In the top life history, the dataset only exists in the dataset 1 pipe and so all identities coincide. Dataset 1 is a bUnit stage, a data content immutability stage and a dataset. In the middle life history, dataset A persists through the bUnit pipes dataset 1 and dataset 2, but the content changes. So both bUnit stages are also data content immutability stages. In the bottom life history, the dataset persists through three bUnit stages, and the content changes through the first two and persists in the third. This third bUnit, dataset 3, is a bUnit stage and a part of a data content immutability stage with dataset 2. AGu - Chris Partridge possible worlds, counterparts AGu - I removed the evolution arrowAlso as this figure shows upon inspection, there are levels of dataset identity. The objects with lower levels of identity type are either coincident with or part of the higher levels; so a bUnit stage is either coincident with a data content immutability stage or part of one and a data content immutability stages is either coincident with a dataset or part of one.Every possible combination of coincidences is allowed as shown in the table below. In the table, where there is a coincidence, the higher level of identity type is highlighted in green.  typeis alloweddatayesnononoyesyesyesdata content immutability stagenoyesnoyesnoyesyesdata bUnit stagenonoyesyesyesnoyescombinations of coincident identity transformation typesWhere there is a coincidence, each identity type might have a name, if so, this will result in the coincident object having multiple names.An algorithmic notion of identity (and so difference) There are ways of algorithmically calculating identity and immutability for datasets and data items. The details are in the sub-sections and in TtDT - Report - Appendix - bH - bHashing and bSumming.This approach defines transformation in terms of identity and its complement difference. \u2018A difference that makes a difference' can be seen, in this context, as the essence of a transformation. We should not account for a difference that makes no difference as a transformation of interest. By developing a clear algorithmic notion of identity (and so difference) one has the basis for developing automatic accounting processes for checking whether there are differences and so transformations when there should be identity \u2013 or where there are differences that can be summed into an identity.One consequence of using algorithmic identity is that it leads to the existence of intermittent objects. Consider, as shown below, a dataset that is split and then subsequently merged \u2013 where the merged dataset has exactly the \u2018same\u2019 rows as the original dataset. Then as the life history below shows the original and final datasets are stages of the same dataset \u2013 as they have the same data identity. AGu: initial pictureAGu: updated pictureBuilding algorithmic identity-based transparency Building transparency is done in two stages:mapping (and reviewing) the intended pipeline transformations, andtesting the pipeline transformations happen as intended.The transformation mapping has two elements; tracking and tracing.  Tracking finds the intended footprint of the data identities and tracing maps the intended identity transformations.The testing compares the intended tracking and tracing with what happens in the actual runs.As in other sections, more technical details, including most references to useful literature have been relegated to appendices, which are referred to in the text.", "tags": "", "url": "page5769494532.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark56'>TtDT - Report - Building resilient dataset transformation transparency</h1>", "text": "IntroductionbCLEARer stage pipelines work at the level of datasets rather than dataset collections and so have a structure that offers substantially more opportunities for accounting. In this section the focus is on building transformation transparency for those datasets. After a section setting the scene, the following topics are covered:firstly, a general notion of algorithmic identity, from which difference and so transformation can be established, andsecondly, how to implement transparency in the individual bCLEARer stage pipelines through tracking, tracing and testing transformations.The discussion of the second topic is divided into three sections:mapping tracking of intended identities mapping tracing of intended changing identitiestesting tracking and tracing for actual executionsSetting the sceneThe bCLEARer stage is designed as a sequence of bUnit process types \u2013 the bUnit flow \u2013 which may or may not be organised into sub-pipelines within the stage. It is important that this flow is not only transparent, open to inspection, but that the transparency is also resilient in the face of change. A bUnit process type can be characterised as a type of process that consumes one or more (input) dataset types and produces one or more new (output) dataset types. At this level, unlike the higher levels which work with dataset collections, dataset types are individuated, where each dataset type is picked out as separate. So process types have associated flow mappings \u2013 mappings from the individual input dataset type and to the individual output dataset type.The components of the bUnit flow, the bUnit process and dataset types, are identified using a name, typically reflecting its function, that is unique within the bUnit, which is often supplemented with a project-wide (code) identifier. When the bUnit dataset flow is run, the run is given an identifier. The datasets and processes in the run are identified by the combination of the type identifier and the run identifier.In the context of bUnit flow, a dataset is a collection of data items in a common format \u2013 where a data item is a single unit of data. A tabular row dataset \u2013 a common type of dataset \u2013 is a table where the data items are the rows (these rows have an internal structure/content stored in cells based upon the table's columns). Another common case is a tabular cell dataset where the data items are the table\u2019s cells themselves. One can visualise the bUnit flow in a number of ways. Firstly, from the perspective of the process types \u2013 see below. Secondly from the perspective of process and dataset types, where the pipes are adorned with a dataset icon \u2013 see below. And finally from a pure dataset type perspective, a bUnit dataset flow, showing a sequence of datasets \u2013 see below.A general notion of algorithmic dataset identityIn the bCLEARer stage pipeline design process, one defines (and so, in this scheme of things, gives identity to) bUnit dataset types and their associated bUnit filter types. In the diagram below, \u2018dataset 1\u2019 and \u2018dataset 2' are bUnit dataset types associated with bUnit filter type 'filter A\u2019. These names help humans keep track of the identities for the bUnit datasets and process types. In the implementation there will be internal identifiers corresponding to these for the computer to use.Filters are where transformations happen. From the perspective of datasets, we can be more specific and locate it as a property of the relation between a filter's input dataset and output dataset \u2013 a filter-dataset flow. This is perhaps more easily visualised when the flow is seen from the dataset perspective, as in the diagram below. So the starting point for inspecting dataset transformations is these filter-dataset flows. To make them more concrete we can list them in a table \u2013 as done below. bUnit processbUnit inputbUnit outputfilter Adataset 1dataset 2To enable tracking and tracing of dataset transformations one firstly needs to identify the types of dataset identities that are to be tracked and traced. Then one can ask for their filter-dataset flows, showing where these identities are preserved.The two core types of algorithmic dataset identity bCLEARer works with at the moment are:dataset item identitydataset item immutable stage identityAs these are all algorithmic identities, they can be tested automatically. bCLEARer implements these algorithms using counts, sums and hashes (which are described here: TtDT - Report - Appendix - bH - bHashing and bSumming) and stores them as metadata on all the bUnit datasets. This makes it easy to test for unexpected differences.These identities are described below.Dataset item identityThe first type of algorithmic identity is dataset item identity. This is based upon immutability of the collected data items' identities. In a bUnit filter where an input dataset type is intended to have a corresponding output dataset type that collects exactly the \u2018same\u2019 data items \u2013 where sameness is based upon data item identity \u2013 then they share dataset item identity. The content of the data items may change \u2013 for example, a column may be dropped \u2013 but this does not affect the dataset identity. However, a merge or split of a dataset, where the collected data items change will not qualify for dataset item identity.Reconsider the example above. Assume we intend that filter A preserves dataset item identity, then we could record the transformation characteristics to the filter-dataset flow as shown in the table below. bUnit processbUnit inputbUnit outputidentityfilter Adataset 1dataset 2dataset item identityWhere a filter-dataset flow has this characteristic, when it is executed we test the identity. Where there is a difference, this is reported and should be investigated \u2013 we discuss this further in the testing section below.Of course, it is possible that there are multiple item identities that that dataset is tracking, but we only consider the case where there is one here.Dataset item immutable stage identityThe second type of algorithmic identity is dataset item immutable stage identity \u2013 this is based upon immutability of the collected data items' content (including their identities). This follows a similar pattern to the identity described above.In a bUnit filter, such as a pass-through, where an input dataset is intended to have a corresponding output dataset type that collects exactly the \u2018same\u2019 data items with their content unchanged \u2013 then they share dataset item immutable stage identity. In this case, dropping a column from the dataset would change the contents, so they wouldn\u2019t share this identity.  This can be regarded as a more stringent kind of dataset item identity \u2013 as dataset item immutable stage identity implies dataset item identity. Reconsider again the filter A example. Assume we now intend filter A to preserve dataset item immutable stage identity, then we could record the transformation characteristics to the filter-dataset flow as shown in the table below. bUnit processbUnit inputbUnit outputidentityfilter Adataset 1dataset 2dataset item immutable stage identityWhere a filter-dataset flow has this characteristic, when the pipeline is executed we test for this identity. Where there is a difference, this is reported and should be investigated \u2013 we discuss this further in the testing section below. In practice, we will need to differentiate between cases where we expect the dataset and associated data items' identity to always change and where they may change, but don\u2019t necessarily. in this exposition we gloss over this distinction.Where a dataset is processed and the input and output versions both collect the \u2018same\u2019 data items with the same content, then they are the same dataset immutable stage. Where the content of a data item changes \u2013 for example, a column is dropped \u2013 this marks the end of the dataset immutable stage but does not affect the dataset identity. The obvious candidate for stage identity based upon content immutability is the maximal content of the dataset. For example, in the case of tables, this would be all the data columns. There will be cases where the content naturally divides into sub-content and so can be usefully tracked in finer detail. However we only consider the case where there is a single (maximal) notion of content here.Tracking intended dataset identitiesIn this context, tracking means following the intended flow of the two identities through the bUnit pipeline. In other words, for a particular dataset identity, which bUnit datasets (pipes) it is intended to pass through. This involves mapping where it is intended to be preserved across bUnit filters. We describe this in more detail in this section.Tracking a simple pass-through Consider first a simple pass-through pipeline visualised in the figure below.This gives rise to the filter-dataset transformation characteristics in the table below.bUnit processbUnit inputbUnit outputidentity levelpass-throughdataset 1dataset 2dataset item identitypass-throughdataset 1dataset 2dataset item immutable stage identityFrom this table we can infer that there is a dataset, of which dataset 1 and dataset 2 are bUnit filter stages \u2013 dataset A. We can also infer that this dataset is its own immutable stage, as it is immutable throughout its life. This means it has two names. We tend to use the shorter names in diagrams and have both in one of the tables for reference (here you can find both names in a later table). This structure can be visualised as a tracking life history \u2013 as shown in the figure below.The tracking is recorded in the component structure. In this case, where the bUnit dataset is a stage in the life of the larger dataset. This structure can be recorded in a tracking table, such as the one below.compositetracked component dataset Adataset 1 / dataset A bUnit stage dataset 1dataset Adataset 2 / dataset A bUnit stage dataset 2The stage succession structure can also be recorded in a tablebeforeaftertypedataset 1 / dataset A bUnit stage dataset 1dataset 2 / dataset A bUnit stage dataset 2bUnit stageTracing intended dataset identitiesIn this context, tracing means identifying the intended flow of transformation based upon multiple tracked identities. This involves mapping where the bUnit filters intend a transformation. We describe this in more detail in this section.Tracing simple dataset stage successionsIt can be intended that a dataset remain immutable throughout its life. Or, it can be intended that datasets can change (be mutable). In the bCLEARer stage pipeline, the changes translate into a series of immutable stages.  The tracing marks out the sequence of stages.Consider the simple single filter pipeline in the figure below.Assume filter A preserves dataset identity, but transforms the content (in some way). This gives rise to the mapping table below.bUnit processbUnit inputbUnit outputidentity levelfilter Adataset 1dataset 2dataset item identityFrom this one can infer the existence of dataset A and its two immutable stages that are identical with their corresponding bUnit datasets. We can record this in an existence table like that below.Identity datasetcomponentdataset Wdataset 1 / dataset W immutable bUnit stage dataset 1dataset Wdataset 2 / dataset W immutable bUnit stage dataset 2For tracing purposes, one also needs to identify the succession transformation. This can be inferred algorithmically from the previous tables. One can record them in an succession table like that below.prior dataset stagepost dataset stagebUnit processdataset W immutable bUnit stage dataset 1dataset W immutable bUnit stage dataset 2filter AThis trace can be shown visually in a life history.Tracing simple dataset successionsConsider a simple single filter pipeline whose bUnit filter is designed to take a dataset as input and output, based upon this, a different dataset. Then the dataset identity table is empty, as no identities are preserved. But there is an intended transformation, the emergence of a new dataset, that needs to be traced. Reconsider the pipeline in the figure above. In this example, assume dataset 2 is a new dataset, different from dataset 1. In this simple case, the bUnit dataset flow links the two (distinct) bUnit datasets.For tracing purposes one also needs to identify the dataset emergence transformation \u2013 the links from the emerging dataset back to the dataset it is immediately dependent upon. This can be inferred algorithmically from the filter and recorded in an mapping table like that below.prior dataset post dataset bUnit processdataset 1dataset 2filter AThis trace can be shown visually in a life history.Tracking and tracing branchesThe bCLEARer stage pipeline flow can split-and-merge, which creates the possibility for the dataset identities to split and merge as well. Consider the pipeline in the figure below. Assume the filters (whatever they are) just preserve data item identity, so not immutable stage identity. This gives rise to the mapping table below.bUnit processbUnit inputbUnit outputidentity levelfilter Xdataset 1dataset 2dataset item identityfilter Ydataset 1dataset 3dataset item identityfilter Zdataset 2dataset 4dataset item identityfilter Zdataset 3dataset 4dataset item identityThe lack of dataset item immutable stage identity implies that there is a dataset item immutable stage difference. The dataset item identity implies the existence of a dataset persisting through the bUnit dataset flow \u2013 dataset A \u2013 of which the bUnit datasets are components \u2013 as shown in the table below.compositecomponentdataset Adataset 1 / dataset A immutable bUnit stage dataset 1dataset Adataset 2 / dataset A immutable bUnit stage dataset 2dataset Adataset 3 / dataset A immutable bUnit stage dataset 3dataset Adataset 4 / dataset A immutable bUnit stage dataset 4The earlier mapping table provide the basis for immutable stage tracing relations between the bUnit datasets. The resulting life history below visualises the tracking \u2013 using components, and tracing \u2013 using arrows.  Or it can be recorded in a table, such as the one below.compositetracked componentdataset Adataset 1 / dataset A immutable bUnit stage 1dataset Adataset 2 / dataset A immutable bUnit stage 2dataset Adataset 3 / dataset A immutable bUnit stage 3dataset Adataset 4 / dataset A immutable bUnit stage 4There are successions between the bUnit dataset stages. These are identified and recorded as part of tracing, which is the topic of the next section.Testing identity Once the intended tracks and traces have been identified (as described in the two previous sections), they can be used when the pipeline is executed to test whether identity is being preserved as intended. The pipeline has been implemented so that every bUnit dataset has immutable metadata; a count, hashsums for item identity and immutable stage identity, and a hash for bUnit stage identity. Each bUnit filter has a corresponding inspection filter that has access to its dataset\u2019s metadata. This is shown graphically in the figure below.The inspection filter uses the dataset metadata as the basis for testing, as described in the next sections.Testing a simple pass throughAssume we have a simple pass-though filter as discussed earlier and shown in the figure below. As it is a pass through, dataset and immutable stage identity are preserved, as shown in the life history.In the pipeline, the associated inspection process has access to the dataset metadata \u2013 as shown in the figure below.It uses the track and trace maps to test the transformations. In this case, all non-bUnit items of metadata should match (bUnit stage identities should never match).Testing a simple column dropNow assume we have a simple filter where the dataset has its content transformed \u2013 by, for example, dropping a column or two. In this case, dataset identity is preserved, but immutable stage identity is not, as shown in the life history.In the pipeline, the associated inspection filter has access to the dataset metadata \u2013 as shown in the figure below.It uses the track and trace mapping to test the transformations. In this case, only the first item of metadata (identity_hashsum) should match.Testing a simple dataset splitTesting splits (and merges - see below) requires a little more calculation than simple matching. Consider the split shown in the figure below. In this case, the sum of the two output items should match the input items.Testing a simple dataset merge Consider the merge shown in the figure below. In this case, the input items should match the sum of the two output items.Expanding the testingWe have only covered a small range of the possible tests that can be done with bCLEARer stage pipelines. But hopefully this is enough to give a good idea of the kinds of test that are feasible. ", "tags": "", "url": "page5765136857.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark74'>TtDT - Report - Building resilient data item transformation transparency</h1>", "text": "IntroductionIn this section the focus is on building transformation transparency for data items. This is done in the context of the bCLEARer stage pipeline datasets. As in the previous section, after a section setting the scene, the following topics are covered.Firstly, a general notion of algorithmic identity, from which difference and so transformation can be established, andSecondly, how to implement transparency in the individual bCLEARer stage pipelines through tracking, tracing and testing transformations.And, as before, the second topic is divided into three parts:mapping tracking of intended identities mapping tracing of intended changing identitiestesting tracking and tracing for actual executionsSetting the sceneThe previous section looked at the bUnit flow as a sequence of bUnit process types \u2013 and how, at the dataset level, one could make this not only transparent, open to inspection, but also resilient in the face of change. It characterised a bUnit process type as a type of process that consumes one or more (input) dataset types and produces one or more new (output) dataset types.The same pattern is recapitulated at the finer-grained level of data items.  Each bUnit dataset is a collection of data items \u2013 the bUnit data items. The bUnit item process type consumes one or more (input) data item  types and produces one or more new (output) data items. So item process types have associated item flow mappings \u2013 mappings from the individual input data items and to the individual output data items. One can visualise this in a similar way to datasets as in the figure below.A general notion of algorithmic data item identityTo enable tracking and tracing (of algorithmic data item identities) one firstly needs to identify the data item identities to track and trace. Then one maps them onto the bUnit item identities in the bCLEARer stage pipelines. This gives one a basis for testing the tracking and tracing.The two core types of algorithmic data item identity bCLEARer works with at the moment are the same as for datasets:data item identitydata item immutable stage identity These identities are described below.Data item identityThe first type of algorithmic identity is data item identity.  Whenever a data item comes into the scope of the pipeline, whether being passed into the pipeline from a data source or created by a filter in the pipeline, it is allocated a unique data item identity. It is the responsibility of the first filter to handle the data item to generate the identity and attach it to the data item.The specific algorithm is selected during the bCLEARer stage pipeline design process. There are various ways of doing this and the choice will depend upon the context. In the case of tables, this is usually based upon a primary key of some kind. If there are no keys, then it can be based upon something as simple as a row count. The ways bCLEARer implements the algorithm is described here: TtDT - Report - Appendix - bH - bHashing and bSumming, often these are hashes. The identity travels along the pipeline flow and is stored in subsequent versions of the data item \u2013 in  subsequent datasets.  It is the responsibility of the filters that write data items to ensure when outputting one that the identity is passed through. Of course, it is possible that there are multiple data item identities that need to be tracked, but we only consider the case where there is one here.Data item immutable stage identityThe second type of algorithmic identity is data item immutable stage identity \u2013 this is based upon immutability of the data items' content (including their identities). As with the previous identity, whenever a data item comes into the scope of the pipeline, whether being passed into the pipeline from a data source or created by a filter in the pipeline, it is allocated a unique data item immutable stage identity. It is the responsibility of the first filter to handle the data item to generate the identifier and attach it to the data item.The specific algorithm for this identity is selected during the bCLEARer stage pipeline design process. There are various ways of doing this and the choice will depend upon the context. In the case of tables, this is usually based upon the content of a row. The ways bCLEARer implements the algorithm is described here: TtDT - Report - Appendix - bH - bHashing and bSumming, often these are hashes. This follows a similar pattern to the other identity described above.Where the filter does not change the content of a data item, then this identity travels along the pipeline flow and is stored in subsequent versions of the data item \u2013 in subsequent datasets.  It is the responsibility of the filters that write data items to ensure when outputting one that the identity is passed through. However, where the filter does not change the content of a data item, a new identity needs to be algorithmically calculated and attached to the data item.As noted previously, the obvious candidate for a data item\u2019s stage identity based upon content immutability is its maximal content. For example, in the case of tables, this would be all the columns in the row. There will be cases where the content naturally divides into sub-content and so can be usefully tracked in finer detail. However we only consider the case where there is a single (maximal) notion of content here.Relation to dataset identitiesData item identity and dataset identity are closely linked. The intended dataset identity depends upon the intended data item identity. Hence, one can infer intended data item identity from intended dataset identity \u2013 if dataset identity is preserved then data item identity must be too. We identified the possible routes for dataset transformations as the filter's input/output dataset combinations \u2013 a filter-dataset flows. Data item transformations have the same routes at the finer granularity of items \u2013 filter-data-item flows. A filter-dataset flow that preserves dataset identity will include finer grained filter-data-item flows that preserve data item identity. But there are cases (dataset splits and merges are an example) where dataset identity is not preserved, but there are finer grained filter-data-item flows that preserve data item identity \u2013 we will look at examples later in this section.Representing identities as metadataIn our diagrams, we will represent the identities as metadata in two ways - as shown below. On the left, the metadata is represented using a 'tag', on the right as columns in a metadata table.Tracking intended data item and stage identitiesIn this context, tracking means identifying the intended flow of the two identities through the bUnit pipeline \u2013 for a particular data item identity, which bUnit datasets it is intended to overlap, to have as stages.  This involves mapping where it is intended to be preserved across bUnit filters. We describe this in more detail below reusing using examples introduced for dataset identity.Tracking a simple pass-through We start with a simple pass-through pipeline example that preserves dataset item identity and dataset item immutable stage identity. The pass-through filter will read a data item from the input dataset and write the same data item with the same content to the output dataset. This implies that both data item identities are preserved. We can visualise that using an example diagram such as that below.Both data items have their three identities as metadata. And as the figure shows all non-bUnit items of metadata should match (bUnit stage identities should never match).The specific data item identities depend upon the contents of the input dataset, so we can record the general transformation characteristics to the filter-data-item flow as shown in the table below. bUnit processbUnit input datasetbUnit output datasetidentityfilter Adataset 1dataset 2data item identityOr, when we know the specifics, as we do for the examples as shown in the table below. bUnit processbUnit input datasetbUnit output datasetbUnit data itemidentityfilter Adataset 1dataset 2data item 1data item identityThis specific flow can also be visualised in a data item life history. Understandably, this looks remarkably similar to the dataset life history for the same example (provided in the last section).Tracing intended data item and stage identityAs at the dataset level, data item tracing means identifying the intended flow of transformation based upon the two tracked identities \u2013 item and stage. This involves mapping where the bUnit filters intend a transformation. Tracing simple data item stage successionsAs noted above, data item identity and dataset identity are closely linked.  Where it is intended that a dataset remain immutable throughout its life, it is intended that the data items will remain immutable too. Where it is intended that datasets can change (be mutable), it is intended that their data items can too. At the data item level, these changes translate into a series of immutable data item stages. The stage tracing maps out the sequence of stages.Consider a simple single filter pipeline, where the dataset item identity is preserved, but the content is always transformed (in some way - maybe by dropping columns). Then data item identity is also preserved and data item stage identity will change \u2013 this is shown graphically in the figure below.Tracing simple data item successionsTracing data item succession requires more infrastructure. Consider a simple match-and-combine pipeline where items from two datasets are matched and then combined in a new dataset. In order to be able to trace this succession, we need to keep a record of what was matched and combined \u2013 a mapping between the data items. There are quite a few ways that data item identity could be expressed in a simple match-and-combine pipeline. Let\u2019s assume in this case it is intended that the input and output data items be different (data items). So where two different data items are merged to create a new third data item. In this case, the two filter-data-item flows would have no general transformation characteristics - so the table would look like this.bUnit processbUnit input datasetbUnit output datasetidentitysimple match-and-combinedataset 1dataset 3NONEsimple match-and-combinedataset 2dataset 3NONE This could be visualised as a flow like this.Or a life history like this - note the trace links.To persist the trace, the match and combine filter needs to record the data in the table below:processbeforeaftersimple match-and-combinebUnit 123 - bUnit dataset 1bUnit 789 - bUnit dataset 3simple match-and-combinebUnit 456 - bUnit dataset 2bUnit 789 - bUnit dataset 3Tracing and tracking branchesOften the tracks of data item identity intertwine with the traces of data item stage transformation. A good example of this is the simple split-and-then-merge pipeline shown in the figure below.Of the various possible ways data identity could be expressed, assume the filters (whatever they are) just preserve dataset item identity, so not dataset immutable stage identity. This implies they preserve data item identity but not data item immutable stage identity. The flow of an example data item through the pipeline could be diagrammed as shown below.This data item flow can also be visualised in a data item life history.Testing data item identity As with datasets, once the intended data item tracks and traces have been identified (as described in the two previous sections), they can be used when the pipeline is executed to test whether identity is being preserved as intended. Typically, data items tests are finer-grained checks undertaken when the coarser grained dataset tests suggest something is wrong. There are a range of ways in which the tests can be deployed. We describe a few examples below to give a flavour of these.Testing a simple pass-throughConsider a simple single filter pass-though pipeline which, as noted above, is intended to preserve dataset and immutable stage identity. Assume firstly that the inspection process has identified that it has failed the dataset identity test. Then the process can run a finer-grained check on each data item to see whether they track between the datasets. The figure below shows how the results of a check might be visualised.Anne Guinard to update the picture with new discontinuity symbol when validated by Chris Partridge Andrew Mitchell Another way of visualising this is as a life history.Now assume firstly that the inspection process has warranted the dataset item identity, but identified that it has failed the dataset immutable stage identity test. Then the process can run a finer-grained check on each data item to establish the data item identity and then run a check on data item immutable stage identity to identify the items that are not behaving as intended.  The figure below shows how the results of a check might be visualised.Anne Guinard to update the picture with new discontinuity symbol when validated by  Chris Partridge Andrew Mitchell Another way of visualising this is as a life history.Testing a simple dataset split Sometimes, the test does not rely directly on simple data set identity. Dataset splits (and merges) are an example \u2013 where the datasets lose their identity but the data items do not. Consider a simple single filter split pipeline which, as noted above, is intended to preserve dataset and immutable stage identity across the split datasets, but not for the individual datasets. Assume that the inspection process has identified that it has failed the dataset identity test. Then the process can run a finer-grained check on each data item to see whether they track between the datasets. The figure below shows how the results of a check might be visualised.Anne Guinard to update the picture with new discontinuity symbol when validated by Chris Partridge Andrew Mitchell Another way of visualising this is as a data item life history.Expanding the testingChris Partridge  to be added", "tags": "", "url": "page5766316201.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark92'>TtDT - Report - Appendices</h1>", "text": "TtDT - Report - Appendix - Process: principles versus rulesTtDT - Report - Appendix - bH - bHashing and bSummingTtDT - Report - Appendix - cohesion and couplingTtDT - Report - Appendix - separation of concerns principleTtDT - Report - Appendix - immutability and idempotence principleTtDT - Report - Appendix - single-transformation (responsibility) principle (STP)TtDT - Report - Appendix - Aggregated S(ingle) S(ource) O(f) T(ruth)TtDT - Report - Appendix - design patterns and anti-patternsTtDT - Report - Appendix - Glossary of Major TermsTtDT - Report - Appendix - Reference IconographyTtDT - Report - Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' Architecture", "tags": "", "url": "page5768675336.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark93'>TtDT - Report - Appendix - Process: principles versus rules</h1>", "text": "Appendix - PvR - Report\nfor review - https://borocvi.atlassian.net/l/cp/1G1tvA1a source: Appendix - PvR - Report \n", "tags": "", "url": "page5769003012.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark95'>TtDT - Report - Appendix - bH - bHashing and bSumming</h1>", "text": "Include - Appendix - bH - bHashing and bSumming Appendix - bH - Report\nAppendix - bH - bHashing and Summingfor review https://borocvi.atlassian.net/l/cp/tCweBUBj  source: Appendix - bH - Report \n", "tags": "", "url": "page5768839184.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark97'>TtDT - Report - Appendix - cohesion and coupling</h1>", "text": "Appendix - Cohesion and coupling principle - Report\nFor review https://borocvi.atlassian.net/l/cp/CrjojeAA see also: Appendix - Separation of concerns principle - Report source: Appendix - Cohesion and coupling principle - Report \n", "tags": "", "url": "page5772804097.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark99'>TtDT - Report - Appendix - separation of concerns principle</h1>", "text": "Appendix - Separation of concerns principle - Report\nSeparation of concerns principle, for review Separation of concerns - Basic Content see also: Appendix - Cohesion and coupling principle - Report source: Appendix - Separation of concerns principle - Report \n", "tags": "", "url": "page5772804106.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark101'>TtDT - Report - Appendix - immutability and idempotence principle</h1>", "text": "Appendix - Immutability and idempotence principle - Report\nImmutability and idempotence principlefor review https://borocvi.atlassian.net/l/cp/gEeJFHA7 See also: https://boroengineering.atlassian.net/wiki/spaces/BKB/pages/1917255836/idempotence source: Appendix - Immutability and idempotence principle - Report \n", "tags": "", "url": "page5772869633.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark103'>TtDT - Report - Appendix - single-transformation (responsibility) principle (STP)</h1>", "text": "Appendix - Single-transformation (responsibility) principle (STP)", "tags": "", "url": "page5772804114.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark104'>TtDT - Report - Appendix - Aggregated S(ingle) S(ource) O(f) T(ruth)</h1>", "text": "Appendix - Aggregated SSOT principle - Report\nsource: Appendix - Aggregated SSOT principle - Report\n", "tags": "", "url": "page5773328385.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark105'>TtDT - Report - Appendix - design patterns and anti-patterns</h1>", "text": "Appendix - Design patterns and anti-patterns - Report\nDesign patternA https://en.wikipedia.org/wiki/Design_pattern \u2018is the re-usable form of a solution to a design problem\u2019.'Each pattern describes a problem that occurs over and over again in our environment, and then describes the core of the solution to that problem, in such a way that you can use this solution a million times over, without ever doing it the same way twice.'The range of situations in which a pattern can be used is called its context.In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. It is not a finished design that can be transformed directly into source or machine code. Rather, it is a description or template for how to solve a problem that can be used in many different situations. Design patterns are formalized best practices that the programmer can use to solve common problems when designing an application or system.See also: https://en.wikipedia.org/wiki/Software_design_pattern Anti-patternAn https://en.wikipedia.org/wiki/Anti-pattern in software engineering, project management, and business processes is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive.As opposed to a bad practiceAn anti-pattern is a commonly-used process, structure or pattern of action that, despite initially appearing to be an appropriate and effective response to a problem, has more bad consequences than good onesAnother solution exists to the problem the anti-pattern is attempting to address. This solution is documented, repeatable, and proven to be effective where the anti-pattern is notsource: Appendix - Design Patterns and Anti-patterns - Report \n", "tags": "", "url": "page5775982593.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark108'>TtDT - Report - Appendix - Glossary of Major Terms</h1>", "text": "TtDT - Report - Appendix - Glossary of Major Terms - ReportTtDT - Report - Appendix - Glossary of Major Terms - Research", "tags": "", "url": "page5780340771.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark109'>TtDT - Report - Appendix - Glossary of Major Terms - Report</h1>", "text": "Pipeline architectureTermDescriptionpipeline architectureCPa - also known as \u2018pipe and filter\u2019 architecturebCLEARer PipelinegateCPa - short for information gatefilterpipesub-pipelinenested pipelineIdentity transformation managementTermDescriptiontracktracetestbUnit flowfilter-dataset flowfilter-data-item flowNested levelsTermDescriptionthin-slicethin-slice setbUnitbCLEARer stagebCLEARer sequenceDataTermDescriptiontransformationtransparencyinspectionidentitysource: TtDT - Report - Appendix - Glossary of Major Terms ", "tags": "", "url": "page5793284135.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark114'>TtDT - Report - Appendix - Glossary of Major Terms - Research</h1>", "text": "Preparatory spreadsheet for the glossaryprep_glossary_appendix_v\u2026\nFrom AMi - others to add?datasetdata itemdataset collectionitem identityimmutable stage identity\n\nFrom AGuas these appear in the iconography appendix, suggestion to add a definition for:nested pipeline diagram: a nested pipeline diagram shows the stages in the evolution of a sub-pipeline into a nested pipeline; it represents the &quot;external layer&quot; of the pipeline at each stagethe pipeline chunk that encapsulates filter and pipe components at a certain stage in the evolution aligns with the components at the previous stage. The nested pipeline diagram and the nesting diagram provide complementary views on nesting. The nested pipeline diagram represents the &quot;external layer&quot; of the pipeline at each stage of the nesting evolution, while the nesting diagram represents the post-nesting stage only, with a view on the external layer and internal layer(s) of the pipeline.  nesting diagram: a &quot;see-through&quot; representation of the different levels of nesting within a nested pipeline. The nesting diagram and the nested pipeline diagram provide complementary views on nesting. The nested pipeline diagram represents the &quot;external layer&quot; of the pipeline at each stage of the nesting evolution, while the nesting diagram represents the post-nesting stage only, with a view on the external layer and internal layer(s) of the pipeline.  \n", "tags": "", "url": "page5793218610.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark115'>TtDT - Report - Appendix - Reference Iconography</h1>", "text": "TtDT - Report - Appendix - Reference Iconography - ReportTtDT - Report - Appendix - Reference Iconography - Research", "tags": "", "url": "page5784010894.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark116'>TtDT - Report - Appendix - Reference Iconography - Report</h1>", "text": "This appendix presents the iconography used in the report; it follows a main structure in three (four ? add interdiagram mapping?) parts:pipeline architecture iconsPipe-related iconsFilter-related iconsPipeline iconsNesting iconslife history iconsIdentity tracking/tracing - generalIdentity tracking/tracing - in life history space-time diagramsicons common to the two areasEach leaf in the structure is organised in two sub-sections:a presentation of atomic iconsa presentation of composite icons, i.e. icons that are obtained through a combination of atomic iconsWithin these sub-sections:icons are listed according to the alphabetic order of their names.Where appropriate, leaf or main sections are complemented with examples of diagrams that illustrate the in-context use of icons.TtDT - Report - Appendix - Reference Iconography - Report - Pipeline architectureTtDT - Report - Appendix - Reference Iconography - Report - Life historyTtDT - Report - Appendix - Reference Iconography - Report - Inter-diagram mappingTtDT - Report - Appendix - Reference Iconography - Report - Common", "tags": "", "url": "page5783355393.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark117'>TtDT - Report - Appendix - Reference Iconography - Report - Pipeline architecture</h1>", "text": "CPa: what should the order of sections be? would pipeline not go before pipes and filters? Or do you build from the atoms up? If so, why not call a section base components with subsections for pipes and filters? =&gt; AGu - see info about appendix structure in TtDT - Report - Appendix - Reference Iconography - Report - Shell bCLEARer - Confluence (atlassian.net)CPa: generally, I think far too many columns - also columns 1 and 3 seem to repeat (DRY/WET). So  what would be a better set of columns? Maybe make the columns, sections within a column? =&gt; AGu =&gt; limited number of columnsCPa: Are we using an alphabetic ordering convention - within the sections? - maybe say this. =&gt; AGu - added info about appendix structure in TtDT - Report - Appendix - Reference Iconography - Report - Shell bCLEARer - Confluence (atlassian.net)CPa: Should we have a Table of Contents? See below - can we exclude the rows?\n\nPipe-related icons\n\nComponents\nComposites\n\n\nFilter-related icons\n\nComponents\nComposites\n\n\nPipeline icons\n\nComponents\n\n\nNesting icons\n\nComponents\n\n\nExamples\n\nPerspectives\nSimple pipeline\nNesting\n\n\n\nPipe-related iconsComponentsiconography object namesiconography objectsnotesdata itemdatasetThis icon represents a dataset. It is typically enriched with the dataset\u2019s metadata columns and used when illustrating the impact of a transformation on a dataset\u2019s data items and their identities (see example \u2026)  [CPa: Ummm, is this a pipe, or the dataset itself? I\u2019m not sure.]dataset (pipe) &amp; variantsThe following section presents the standard icon of a pipe that carries a dataset and its variant representations.Icons in this list can be used on their own (see example given in the data type perspective glossary entry) or in conjunction with a pipe arrow icon (see example given in the process type perspective glossary entry). They can optionally be named - see example given for the dataset (pipe) icon below.dataset (pipe)The standard icon (and its variants) can optionally be named, e.g.:dataset (pipe) - variant - with data itemsThis icon is a variation on the dataset (pipe) icon. It uses the dataset icon, adorned with numbered data items. A data item followed by an asterisk is an item whose immutable content changed through a transformation.dataset (pipe) - variant - with   implicit split and data itemsThis icon is a variation on the dataset (pipe) - variant - with data items icon.  It presents a dotted line delineation anticipating a split of the dataset or illustrating the result of a merge.dataset collection (pipe)This icon represents a pipe that carries a collection of datasets. It can be used on its own (see example given in the data type perspective glossary entry) or in conjunction with a pipe arrow icon (see example given in the process type perspective glossary entry).It can optionally be named, e.g.:gatemetadata (columns) - data itemThis icon typically enriches the data item icon.metadata (columns) - datasetThis icon typically enriches the dataset icon.metadata (label) - data itemThis icon typically enriches the data item icon.metadata (label) - datasetThis icon typically enriches the dataset (pipe) icon and its variants.pipe arrowCompositesiconography object namesiconography objectsnotesdata item + metadata (columns) - data itemThis composite icon represents a data item and its metadata.data item + metadata (label) - data itemThis composite icon represents a data item and its metadata.dataset (pipe) &amp; variants + metadata (label) - data setThis composite icon represents a dataset and its metadata.The composite icon in the example on the left is based on the dataset (pipe) - variant - with data items icon. The standard and other variant versions of the dataset (pipe) icon can also be similarly combined with the metadata (label) - data set icon.dataset (pipe) &amp; variants + pipe arrowThis composite icon represents a pipe that carries a dataset.The variant icons of the dataset (pipe) icon can similarly be combined with the pipe arrow. (although we don\u2019t have any case in the report)dataset + metadata (columns) - datasetThis icon represents a dataset and its metadata.dataset collection (pipe) + gateThis icon represents a gated dataset collection.dataset collection (pipe) + pipe arrowThis composite icon represents a pipe that carries a dataset collection.Filter-related iconsComponentsiconography object namesiconography objectsnotesfilter (data-type perspective)This icon represents a filter; it is used in a data-type perspective.filter (process-type perspective)This icon represents a filter; it is used in a process type perspective.manualThis icon is typically used as an adornment of a filter; it symbolises that the filter involves manual work.Compositesiconography object namesiconography objectsnotesfilter (process-type perspective) &amp; manualThis composite icon represents the \u201CLOAD\u201D bCLEARer stage which typically involves manual work.note AGu - nesting colouring which applies to filters has been moved to nesting sectionPipeline iconsComponentsiconography object namesiconography objectsnotesprocess timeThis icon represents the left to right direction of the pipeline flow - pipes passing data and filters transforming it. See example of use here.Nesting iconsComponentsiconography object namesiconography objectsnotescontainerSee example of use here.encapsulation XXXX. See example of use here.evolution stageThis icon is used to present a stage in the evolution of the pipeline flow. Typically, evolution stage icons are arranged in a vertical series to represent snapshots of the pipeline and used conjointly with the evolutionary time icon. See example.evolutionary timeThis icon shows the direction of evolution of a pipeline (as pipes and filters may be added, removed - or changed, for instance). It is typically used conjointly with a series of evolution stage icon.It may be named differently according to the context: for instance, as \u201Cdesign evolution\u201D, \u201Cpipeline evolution\u201D \u2026nesting colouringThis palette applies to the filter (process-type perspective) icon and the container icon to signify the bCLEARer nesting level the respective filter / container belong to. See example of use here.sub-pipelineXXXX. See example of use here.ExamplesPerspectivesexample namesnotes and examplesprocess type perspectivenotes: in a process type perspective, pipes are represented with the pipe icon (adorned or unadorned with a data icon) and filters are represented with the filter (process-type perspective) icon.example 1:example 2:data type perspectivenotes: in a dataset type perspective, pipes are represented with the data icons (dataset collection icon or a dataset icon). Filters are represented with the filter (data-type perspective) icon.example:Simple pipelineA simple pipeline diagram is obtained by combining pipe-related and filter-related icons, according to one of the above-listed perspectives. It may include a process time icon.example namesnotes and examplessimple pipeline diagram (process type perspective)simple pipeline diagram (data type perspective)Nestingexample namesnotes and examplesnested pipeline diagramnotes: A nested pipeline diagram shows the evolution of a sub-pipeline into a nested pipeline; it represents the &quot;external layer&quot; of the pipeline at each stage.The pipeline chunk that encapsulates filter and pipe components at a stage in the evolution aligns with the encapsulated components from the previous stage.The diagram typically involves the evolution, sub-pipeline and encapsulation icons.example:nesting diagramnotes: a nesting diagram is a &quot;see-through&quot; representation of the different levels of nesting within a nested pipelineThe diagram includes container icons.example:corresponding nested pipeline and nesting diagramsnotes: the nested pipeline diagram and the nesting diagram provide complementary views on nesting. The nested pipeline diagram represents the &quot;external layer&quot; of the pipeline at each stage of the nesting evolution, while the nesting diagram represents the post-nesting stage only, with a view on the external layer and internal layer(s) of the pipeline.  below is an example of corresponding nested pipeline and nesting diagrams:", "tags": "", "url": "page5797249025.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark170'>TtDT - Report - Appendix - Reference Iconography - Report - Life history</h1>", "text": "\n\nIdentity tracking/tracing - general\n\nComponents\nIn-context examples\n\n\nIdentity tracking/tracing - in life history space-time diagrams\n\nComponents\nIn-context examples\n\n\n\nIdentity tracking/tracing - generalComponentsiconography object namesiconography objectsnotesinspectionsingle object:this rectangular shape with the \u201Cinspection\u201D name surrounds processes that relate to data inspectionsee in-context exampletracking/mapping single object:The tracking/mapping arrow represents the trace of data. Where there is a discontinuity in the trace, the arrow is \u201CT-shaped\u201D as shown in the in-context exampleIn-context examplesdiagramsnotes and examplesinspectiondataset tracking/mapping Identity tracking/tracing - in life history space-time diagramsComponentsiconography object namesiconography objectsnotesidentity transformation types paletteThis palette applies to space-time extensions represented in a life history space-time diagram. It can be used to track identities visually.trace linkThis icon can be used to trace identities in a life history space-time diagram. In-context examplesdiagramsnotes and examplesBORO Space Time Mapnote: the life history space-time diagram iconography is based on the BORO Space Time Maps (bSTM) iconography, which provides a way to visualise four-dimensional objects. A typical space-time map:contains a space-time map (STM) reference frame to locate objects in spacetimeshows STM objects' space time boundaries (or extensions).shows STM objects' namesExample:Where space-time boundaries coincide (in the example below, the space-time extension of containing object #1 coincides with the space-time extension of contained objects #1 and #2), an explanatory gap between boundaries is used.life-history space time diagramA life history space-time diagram is typically a BORO Space Time Map enriched with tracking/tracing icons.In the example below:the trace link icon between the extensions of dataset 1 and dataset 2 indicates the predecessor and successor in the transformationspace-time extensions for dataset 1, dataset 2 and dataset A are coloured according to the identity transformation types palette.", "tags": "", "url": "page5796298761.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark184'>TtDT - Report - Appendix - Reference Iconography - Report - Inter-diagram mapping</h1>", "text": "The pipeline architecture and life history iconographies are used conjointly in pipeline architecture-linked life history diagrams to illustrate the footprint of individual bCLEARer stage pipelines in terms of data identities.diagramsnotes and examplespipeline architecture-linked life history diagramA pipeline architecture-linked life history diagram consists of a data type perspective pipeline diagram and the representation of the pipeline\u2019s impact in terms of data identities in a life history diagram.Explosion lines are used in a pipeline architecture-linked life history diagram to map the data appearing in the pipeline diagram to their footprint in terms of data identities in the life history diagram.example (dataset pipeline):example (data item pipeline):", "tags": "", "url": "page5796299378.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark186'>TtDT - Report - Appendix - Reference Iconography - Report - Common</h1>", "text": "Evolutionobjectsnotes and examplesevolutionMiscellaneous", "tags": "", "url": "page5796299991.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark190'>TtDT - Report - Appendix - Reference Iconography - Research</h1>", "text": "AGu - Introductory note for appendix:Two main requirements for any bCLEARer project described in the reportthe nested pipeline architectureprovides the infrastructure in which the data transformations unfoldthe mapping (tracking and tracing) and testing based on the identification of data identitiesensure that transparency is built into the bCLEARer pipelineTo provide a design-level illustration of the implications of these two main requirements, two main set of icons and diagrams have been developed:pipeline architecture iconography; encompasses:pipeline lower level component icons:filter icons, pipe and data iconswhere the data icons present the particularity of having different functions according to a perspective:a function of pipe adornment in a process and data type perspectivea function of pipe, in a data type perspectivethese lower level components can be classified according to their function: pipe or filterpipeline simple diagrams built from the lower level component iconsmore complex pipeline nesting evolution diagrams, that illustrate the nesting structure that characterises the bCLEARer pipelinelife history icons and diagrams:provide a visual way to illustrate the identification, tracing and inspection of flows of data identities for transformation transparency purposesthey are mainly based on the BORO Space Time Maps (bSTM) iconography, which provides an easy way to visualise four-dimensional objectsThe two iconography sets are conjointly used to illustrate the footprint of individual bCLEARer stage pipelines in terms of data identitiessource: TtDT - Report - Appendix - Reference Iconography - Research PipeIconsIcon namesNotespipepipepipe icon adorned with the dataset collection iconpipepipe icon adorned with the dataset iconstart pipepipe icon adorned with the data source iconend pipepipe icon adorned with the data target icongategategate icon adorned with the dataset collection iconFilterIconsIcon namesNotesfilter, also known as processAGu - add bCLEARer palette?filtermanualthis icon adorns filters requiring a manual actionNestingIconsIcon namesNotesExample:sub-pipelineSingle icon:Example:encapsulationContainers are represented with a rectangular shape with no border.The following palette is used for the background colour of the containers in the different levels of the bCLEARer pipeline:containerProcesses are represented with a rectangular shape with rounded corners and a border.The following palette is used for the background colour of processes (filters) in the different levels of the bCLEARer pipeline:process (filter)Axis IconsIcon namesNotesevolutionary timeevolution stageAGu: this is not an axis, but can it sit in the axis section?process timeSpace-time / life historyIconsIcon namesNotesCommonsIconsIcon namesNotes", "tags": "", "url": "page5785092097.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark197'>TtDT - Report - Appendix - Reference Iconography - Report - Pipeline architecture _ archive 30/01/23</h1>", "text": "CPa: what should the order of sections be? would pipeline not go before pipes and filters? Or do you build from the atoms up? If so, why not call a section base components with subsections for pipes and filters?CPa: generally, I think far too many columns - also columns 1 and 3 seem to repeat (DRY/WET). So  what would be a better set of columns? Maybe make the columns, sections within a column?CPa: Are we using an alphabetic ordering convention - within the sections? - maybe say this.CPa: Should we have a Table of Contents? See below - can we exclude the rows?\n\nPipe-related icons\n\ndataset collection (pipe)\ndataset (pipe)\ndata item\nmetadata\npipe\n\n\nFilter-related icons\n\nfilter\n\n\nPipeline\n\nPerspectives\n\nprocess type perspective\ndata type perspective\n\n\nSimple pipeline\n\nsimple pipeline diagram\nprocess time\n\n\n\n\nNesting\n\nNested pipeline diagram\n\nnested pipeline diagram\n\n\nNesting diagram\n\nnesting diagram\ncontainer\n\n\nInter-diagram mapping\n\ncorresponding nested pipeline and nesting diagrams\n\n\n\n\n\nPipe-related iconsobjectsiconography objectsiconography object namesnotesiconography objects adorned with \u2026iconography objects adorning \u2026dataset collection (pipe)[CPa: Ummm, I wonder whether we should have the notion of a data icon with two representations???]dataset collection [CPa: duplication??]The dataset collection icon has the function of pipe adornment in a process type perspective, and the function of a pipe in a data type perspective.[CPa: rewrite:  The icon represents a pipe that carries a collection of datasets. It can used on its own (e.g. in a data type perspective) /// or maybe make this a different entry??? - or in conjunction with a pipe arrow icon (in e.g. a process type perspective, ). As a pipe it can optionally be named.][CPa: Ummm, in practice, we would name the actual dataset collection with a meaningful name - these names are placeholders, aren\u2019t they? ]This icon can be adorned with a &quot;data source&quot; name (respectively &quot;data target\u201D name) when the dataset collection is a data source (respectively, a data target).[Should the following para go here?]The dataset collection icon may be adorned with a gate icon if the data is gated.The dataset collection icon may adorn a pipe icon in a process type perspective.dataset (pipe)dataset [CPa: duplication??]The dataset icon has the function of pipe adornment in a process type perspective, and the function of a pipe in a data type perspective.[CPa: see above][CPa: Ummm, should we describe the base items elsewhere?]Numbered items may be used to adorn the dataset icon, to represent the data items of the dataset. Items followed by an asterisk are items whose immutable content changed through a transformation, e.g.:The dataset icon may be adorned with the metadata (label) icon, e.g.:The dataset icon may be adorned with a dotted line delineation anticipating a split of the dataset or illustrating the result of a merge, e.g.:The dataset icon may adorn a pipe icon in a process type perspective.[CPa: Ummm, is this a pipe, or the dataset itself? I\u2019m not sure.]dataset (with row details)This icon may be adorned with the metadata (columns) icon, e.g.:data itemdata itemThis icon may be adorned with the metadata (label) icon or with the metadata (columns) icon, e.g.:metadataTO BE CONTINUED\u2026pipeNote: A pipe can be represented by a dataset collection icon or a dataset icon in a data type perspective.pipeThis icon may be adorned with a dataset collection icon or a dataset icon, see examples below:gateThe gate icon may adorn a data set collection icon if the data is gated.Filter-related iconsobjectsiconography objectsiconography object namesnotesiconography objects adorned with \u2026iconography objects adorning \u2026filterfilter (process-type perspective)This icon is used in a process type perspective or a process and data type perspectivethe bCLEARer nesting level palette specifies background colours for this icon:the filter (process-type perspective) icon can be adorned with the manual icon, to symbolise that the filter involves manual work, e.g.:filter (data-type perspective)This icon is used in a data-type perspective.manualPipelinePerspectivesobjectsnotes and examplesprocess type perspectivenotes: in a process type perspective, pipes are represented with the pipe icon (adorned or unadorned with a data icon) and filters are represented with the filter (process-type perspective) icon.example 1:example 2:data type perspectivenotes: in a dataset type perspective, pipes are represented with the data icons (dataset collection icon or a dataset icon). Filters are represented with the filter (data-type perspective) icon.example:Simple pipelinediagramsnotes and examplessimple pipeline diagramnotes: a simple pipeline diagram is obtained by combining pipe-related and filter-related icons, according to one of the above-listed perspectives. It may include a process time icon.example 1 (with a process type perspective):example 2 (with a data type perspective):objectsiconography objectsiconography object namesnotesiconography objects adorned with \u2026iconography objects adorning \u2026process timeprocess timeNestingNested pipeline diagramdiagramsnotes and examplesnested pipeline diagramnotes: A nested pipeline diagram shows the evolution of a sub-pipeline into a nested pipeline; it represents the &quot;external layer&quot; of the pipeline at each stage.The pipeline chunk that encapsulates filter and pipe components at a stage in the evolution aligns with the encapsulated components from the previous stage.The diagram includes evolution, subpipeline and encapsulation icons.example:objectsiconography objectsiconography object namesnotesiconography objects adorned with \u2026iconography objects adorning \u2026encapsulationencapsulationsub-pipelinesub-pipelineNesting diagramdiagramsnotes and examplesnesting diagramnotes: a nesting diagram is a &quot;see-through&quot; representation of the different levels of nesting within a nested pipelineThe diagram includes container icons.example:objectsiconography objectsiconography object namesnotesiconography objects adorned with \u2026iconography objects adorning \u2026containercontainerthe bCLEARer nesting level palette specifies background colours for this icon:Inter-diagram mappingdiagramsnotes and examplescorresponding nested pipeline and nesting diagramsnotes: the nested pipeline diagram and the nesting diagram provide complementary views on nesting. The nested pipeline diagram represents the &quot;external layer&quot; of the pipeline at each stage of the nesting evolution, while the nesting diagram represents the post-nesting stage only, with a view on the external layer and internal layer(s) of the pipeline.  below is an example of corresponding nested pipeline and nesting diagrams:", "tags": "", "url": "page5796331521.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark221'>TtDT - Report - Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' Architecture</h1>", "text": "Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' Architecture - Report\nHistoryDoug McIlroy introduced the \u2018pipe-and-filter\u2019 architecture\u00A0in Unix in 1972. Ritchie [ref: Ritchie, 1984] gives a detailed description of this. It subsequently became known as \u2018pipeline\u2019 architecture. Since then it has become an accepted architecture or architectural pattern and documented in the standard text books; the extract below is an example. It is now commonly used in compliers and data engineering.Particular applications often have a more detailed architecture. For example, compilers typically have a three stage pipeline architecture: front-end: parses input language into an intermediate languagemiddle: performs transformations in the intermediate languageback-end: translates the intermediate language into the output languageThe use of an intermediate language allows for reusable architectural components.Extract from Software Architecture in PracticeSoftware Architecture in Practice is one of the standard textbooks on software architecture. It describes architectures in terms of patterns. This extract provides the \u2018standard\u2019 view of pipe-and-filter architecture. Citation: pp. 229 - 321 Len Bass, P. C. a. R. K., 2012. Software Architecture in Practice. 3rd ed. s.l.:Addison-Wesley Professional.13.1. Architectural PatternsPipe-and-Filter PatternContext: Many systems are required to transform streams of discrete data items, from input to output. Many types of transformations occur repeatedly in practice, and so it is desirable to create these as independent, reusable parts.Problem: Such systems need to be divided into reusable, loosely coupled components with simple, generic interaction mechanisms. In this way they can be flexibly combined with each other. The components, being generic and loosely coupled, are easily reused. The components, being independent, can execute in parallel.Solution: The pattern of interaction in the pipe-and-filter pattern is characterized by successive transformations of streams of data. Data arrives at a filter's input port(s), is transformed, and then is passed via its output port(s) through a pipe to the next filter. A single filter can consume data from, or produce data to, one or more ports.Data transformation systems are typically structured as pipes and filters, with each filter responsible for one part of the overall transformation of the input data. The independent processing at each step supports reuse, parallelization, and simplified reasoning about overall behaviour. Often such systems constitute the front end of signal-processing applications. These systems receive sensor data at a set of initial filters; each of these filters compresses the data and performs initial processing (such as smoothing). Downstream filters reduce the data further and do synthesis across data derived from different sensors. The final filter typically passes its data to an application, for example providing input to modelling or visualization toolsKey termstermdescriptionOverviewData is transformed from a system's external inputs to its external outputs through a series of transformations performed by its filters connected by pipes.ElementsFilter, which is a component that transforms data read on its input port(s) to data written on its output port(s). Filters can execute concurrently with each other. Filters can incrementally transform data; that is, they can start producing output as soon as they start processing input. Important characteristics include processing rates, input/output data formats, and the transformation executed by the filter.Pipe, which is a connector that conveys data from a filter's output port(s) to another filter's input port(s). A pipe has a single source for its input and a single target for its output. A pipe preserves the sequence of data items, and it does not alter the data passing through. Important characteristics include buffer size, protocol of interaction, transmission speed, and format of the data that passes through a pipe.Relations The attachment relation associates the output of filters with the input of pipes and vice versa.Constraints Pipes connect filter output ports to filter input ports. Connected filters must agree on the type of data being passed along the connecting pipe.Specializations of the pattern may restrict the association of components to an acyclic graph or a linear sequence, sometimes called a pipeline.Other specializations may prescribe that components have certain named ports, such as the stdln, stdout, and stderr ports of UNIX filters.Weaknesses The pipe-and-filter pattern is typically not a good choice for an interactive system.Having large numbers of independent filters can add substantial amounts of computational overhead.Pipe-and-filter systems may not be appropriate for long-running computations.source: Appendix - The standard 'Pipeline' or 'Pipe-and-Filter' Architecture - Report \n", "tags": "", "url": "page5784338433.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark227'>TtDT - Report - References</h1>", "text": "", "tags": "", "url": "page5766578192.html"},

{"title": "<h1 class='page-title-lvl-cover' id='Bookmark228'>TtDT - Report - Acknowledgements</h1>", "text": "Author(s)Chris Partridge Andrew Mitchell Oscar Xiberta Soto Anne Guinard Mesbah Khan Contributors:Mohammad Yaseen (Unlicensed) ", "tags": "", "url": "page5766545409.html"},

]};
